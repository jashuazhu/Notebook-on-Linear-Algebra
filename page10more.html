<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Chapter 10 - Advanced Applications</title>
  <link rel="stylesheet" href="styles.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['\\[', '\\]'], ['$$','$$']],
        processEscapes: true,
        tags: 'ams'
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <main class="book-page">
    <header class="book-header">
      <p class="lead">Chapter 10</p>
      <h1>Advanced Applications</h1>
      <p class="muted">Spectral graph theory, Kronecker sums, perturbation bounds, and Riemannian optimization.</p>
    </header>

    <nav class="chapter-nav">
      <a href="page9ML.html">&lt;- Previous Chapter</a>
      <a class="index-link" href="index.html">Back to Index</a>
      <span class="disabled">Next Chapter</span>
    </nav>

    <article>
      <div class="grid">

        <section class="card">
          <h2>1) Graph Laplacian Quadratic Form</h2>
          <p><strong>Background &amp; usage.</strong> Links smoothness of signals on a graph to the Laplacian $L$; basis of spectral clustering and semi-supervised learning.</p>
          <div class="formula">
            For the weighted Laplacian $L = D - W$,
            \[
              x^\top L x = \frac{1}{2} \sum_{i, j} w_{ij} (x_i - x_j)^2 \ge 0.
            \]
          </div>
          <details>
            <summary>Proof</summary>
            Expand $x^\top (D - W) x = \sum_i d_i x_i^2 - \sum_{i, j} w_{ij} x_i x_j$ and regroup terms to obtain $\tfrac{1}{2} \sum_{i, j} w_{ij} (x_i - x_j)^2$.
          </details>
          <div class="ex">
            Example: $W = \begin{bmatrix}0 & 1\\ 1 & 0\end{bmatrix}$ gives $L = \begin{bmatrix}1 & -1\\ -1 & 1\end{bmatrix}$. For $x = [1, 2]^\top$, $x^\top L x = 1$.
          </div>
        </section>

        <section class="card">
          <h2>2) Cheeger-Type Connection (Statement)</h2>
          <p><strong>Background &amp; usage.</strong> Relates the spectral gap $\lambda_2(L)$ to graph conductance $\phi(G)$; underpins guarantees for spectral clustering.</p>
          <div class="formula">
            \[
              \frac{\lambda_2(L)}{2} \le \phi(G) \le \sqrt{2 \lambda_2(L)}.
            \]
          </div>
          <details>
            <summary>Proof sketch</summary>
            Rayleigh quotients of the second eigenvector bound conductance from below, while sweep cuts on the eigenvector yield the upper bound via isoperimetric inequalities.
          </details>
          <div class="ex">
            Example: A path of three nodes has Laplacian eigenvalues $0, 1, 3$. Thus $\tfrac{1}{2} \le \phi(G) \le \sqrt{2}$.
          </div>
        </section>

        <section class="card">
          <h2>3) Kronecker Sum for the 2D Discrete Laplacian</h2>
          <p><strong>Background &amp; usage.</strong> Separates variables on grids, enabling fast solvers and eigen-structure reuse.</p>
          <div class="formula">
            If $T_m$ and $T_n$ are the 1D Laplacians, the 2D Laplacian on an $m \times n$ grid is
            \[
              \Delta_{2D} = I_n \otimes T_m + T_n \otimes I_m.
            \]
          </div>
          <details>
            <summary>Proof</summary>
            Index grid nodes lexicographically. The discrete second differences in $x$ and $y$ directions add, and acting on $\operatorname{vec}(X)$ matches the Kronecker sum formula.
          </details>
          <div class="ex">
            Example: With $m = n = 2$ and $T_2 = \begin{bmatrix}-2 & 1\\ 1 & -2\end{bmatrix}$, the Laplacian is
            $\Delta_{2D} = I_2 \otimes T_2 + T_2 \otimes I_2 = \begin{bmatrix}-4 & 1 & 1 & 0\\ 1 & -4 & 0 & 1\\ 1 & 0 & -4 & 1\\ 0 & 1 & 1 & -4\end{bmatrix}$.
          </div>
        </section>

        <section class="card">
          <h2>4) Davis&ndash;Kahan Eigenvector Perturbation (Statement)</h2>
          <p><strong>Background &amp; usage.</strong> Bounds subspace rotation under additive noise; vital in PCA and spectral clustering robustness.</p>
          <div class="formula">
            For symmetric $A$ and $\tilde{A} = A + E$ with eigenvalue gap $\delta$ between the target cluster and the rest,
            \[
              \sin \Theta(\mathcal{U}, \tilde{\mathcal{U}}) \le \frac{\|E\|_2}{\delta}.
            \]
          </div>
          <details>
            <summary>Proof sketch</summary>
            Use resolvent identities and Weyl interlacing to bound the off-diagonal block of the spectral projector, then take operator norms to obtain the angle bound.
          </details>
          <div class="ex">
            Example: $A = \operatorname{diag}(3, 1)$, $E = \begin{bmatrix}0 & \varepsilon\\ \varepsilon & 0\end{bmatrix}$. The gap is $2$, so $\sin \theta \le \varepsilon / 2$. With $\varepsilon = 0.2$, $\theta \le \arcsin(0.1) \approx 5.7^{\circ}$.
          </div>
        </section>

        <section class="card">
          <h2>5) Riemannian Gradient on the Stiefel Manifold</h2>
          <p><strong>Background &amp; usage.</strong> Enables optimization with orthonormal constraints $X^\top X = I$, as in PCA, ICA, and dictionary learning.</p>
          <div class="formula">
            For Euclidean gradient $G = \nabla f(X)$ and $X^\top X = I$, the Riemannian gradient is
            \[
              \operatorname{grad} f(X) = G - X \, \operatorname{sym}(X^\top G), \qquad \operatorname{sym}(M) = \tfrac{1}{2} (M + M^\top).
            \]
          </div>
          <details>
            <summary>Proof</summary>
            The tangent space at $X$ is $\{ Z : X^\top Z + Z^\top X = 0 \}$. Project $G$ onto this space by subtracting the normal component $X \, \operatorname{sym}(X^\top G)$, giving the stated formula.
          </details>
          <div class="ex">
            Example: $X = \begin{bmatrix}1 & 0\\ 0 & 1\\ 0 & 0\end{bmatrix}$, $G = \begin{bmatrix}2 & 1\\ 0 & 3\\ 5 & 4\end{bmatrix}$. Then $X^\top G = \begin{bmatrix}2 & 1\\ 0 & 3\end{bmatrix}$, $\operatorname{sym}(X^\top G) = \begin{bmatrix}2 & 0.5\\ 0.5 & 3\end{bmatrix}$, and the projection yields $\operatorname{grad} f = \begin{bmatrix}0 & 0.5\\ 0 & 0\\ 5 & 4\end{bmatrix}$ in the tangent space.
          </div>
        </section>

      </div>
    </article>

    <nav class="chapter-nav bottom">
      <a href="page9ML.html">&lt;- Previous Chapter</a>
      <a class="index-link" href="index.html">Back to Index</a>
      <span class="disabled">Next Chapter</span>
    </nav>

    <footer class="site-footer">
      <p>Digital Notes Series - Chapter 10</p>
    </footer>
  </main>
</body>
</html>

