<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Chapter 6 - Matrix Factorizations</title>
  <link rel="stylesheet" href="styles.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['\\[', '\\]'], ['$$','$$']],
        processEscapes: true,
        tags: 'ams'
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <main class="book-page">
    <header class="book-header">
      <p class="lead">Chapter 6</p>
      <h1>Matrix Factorizations</h1>
      <p class="muted">Core decompositions for solving systems, least squares, and numerical linear algebra.</p>
    </header>

    <nav class="chapter-nav">
      <a href="page5Calculus.html">&lt;- Previous Chapter</a>
      <a class="index-link" href="index.html">Back to Index</a>
      <a href="page7AdvancedConcepts.html">Next Chapter -&gt;</a>
    </nav>

    <article>
      <div class="grid">

        <section class="card">
          <h2>1) LU with Partial Pivoting: $PA = LU$</h2>
          <p><strong>Background &amp; usage.</strong> LU decomposes a square matrix into lower and upper triangular factors. Pivoting ensures numerical stability and success for all nonsingular $A$.</p>
          <div class="formula">
            For $A \in \mathbb{R}^{n \times n}$ nonsingular there exists a permutation $P$ and triangular $L$ (unit diagonal) and $U$ such that
            \[
              PA = LU.
            \]
          </div>
          <details>
            <summary>Proof sketch (Gaussian elimination)</summary>
            Run elimination on $A$. Whenever a zero or tiny pivot appears, swap rows; each swap is left multiplication by a permutation matrix $P_k$. After $k$ steps, $E_k \cdots E_1 P A = U$ with $E_i$ unit lower triangular. Taking inverses, $P A = (E_k \cdots E_1)^{-1} U = L U$ where $L = E_1^{-1} \cdots E_k^{-1}$.
          </details>
          <div class="ex">
            Example: $A = \begin{bmatrix}2 & 1\\ 4 & 3\end{bmatrix}$. Swap rows: $P = \begin{bmatrix}0 & 1\\ 1 & 0\end{bmatrix}$, $PA = \begin{bmatrix}4 & 3\\ 2 & 1\end{bmatrix}$. Eliminate with $L = \begin{bmatrix}1 & 0\\ \tfrac{1}{2} & 1\end{bmatrix}$, $U = \begin{bmatrix}4 & 3\\ 0 & -\tfrac{1}{2}\end{bmatrix}$, so $LU = PA$.
          </div>
        </section>

        <section class="card">
          <h2>2) QR Factorization: $A = QR$</h2>
          <p><strong>Background &amp; usage.</strong> Expresses columns of $A$ in an orthonormal basis $Q$ with upper triangular $R$. Stable via Householder reflections; central to least squares.</p>
          <div class="formula">
            For $A \in \mathbb{R}^{m \times n}$ with full column rank, there exist $Q \in \mathbb{R}^{m \times n}$ with $Q^\top Q = I$ and upper triangular $R \in \mathbb{R}^{n \times n}$ so that
            \[
              A = QR.
            \]
          </div>
          <details>
            <summary>Proof sketch (Gram&ndash;Schmidt)</summary>
            Apply Gram&ndash;Schmidt to columns $a_i$ to obtain orthonormal $q_i$ and coefficients $r_{ij} = q_i^\top a_j$. Stacking $Q = [q_1, \dots, q_n]$ and the upper triangular $R = [r_{ij}]$ gives $A = QR$. Householder reflections yield the same identity with better stability.
          </details>
          <div class="ex">
            Example: $A = \begin{bmatrix}1 & 1\\ 0 & 1\\ 1 & 0\end{bmatrix}$. Normalize $a_1$ to get $q_1 = \tfrac{1}{\sqrt{2}}(1, 0, 1)^\top$ and $r_{11} = \sqrt{2}$. Orthogonalize $a_2$ against $q_1$, normalize to $q_2$, and assemble $R = \begin{bmatrix} \sqrt{2} & q_1^\top a_2\\ 0 & \|a_2'\| \end{bmatrix}$ so that $A = QR$.
          </div>
        </section>

        <section class="card">
          <h2>3) Cholesky Factorization: $A = LL^\top$</h2>
          <p><strong>Background &amp; usage.</strong> For symmetric positive definite matrices, Cholesky halves the work of LU and preserves symmetry. Used in Gaussian processes, least squares, and SPD systems.</p>
          <div class="formula">
            If $A = A^\top \succ 0$, there exists a unique lower triangular $L$ with positive diagonal such that
            \[
              A = LL^\top.
            \]
          </div>
          <details>
            <summary>Proof sketch (induction)</summary>
            Partition $A = \begin{bmatrix} a & b^\top\\ b & C \end{bmatrix}$ with $a &gt; 0$. Set $\ell_{11} = \sqrt{a}$ and $\ell_{21} = b / \ell_{11}$. The Schur complement $S = C - bb^\top / a$ is SPD, so by induction $S = \tilde L \tilde L^\top$. Then $L = \begin{bmatrix} \ell_{11} & 0\\ \ell_{21} & \tilde L \end{bmatrix}$ satisfies $LL^\top = A$.
          </details>
          <div class="ex">
            Example: $A = \begin{bmatrix}4 & 2\\ 2 & 3\end{bmatrix}$. Compute $\ell_{11} = 2$, $\ell_{21} = 1$, $S = 3 - 1 = 2$, so $\ell_{22} = \sqrt{2}$ and $L = \begin{bmatrix}2 & 0\\ 1 & \sqrt{2}\end{bmatrix}$ with $LL^\top = A$.
          </div>
        </section>

        <section class="card">
          <h2>4) Singular Value Decomposition: $A = U \Sigma V^\top$</h2>
          <p><strong>Background &amp; usage.</strong> SVD diagonalizes any matrix by orthogonal factors. It reveals rank, condition number, and yields optimal low rank approximations.</p>
          <div class="formula">
            For $A \in \mathbb{R}^{m \times n}$ there exist orthogonal $U, V$ and diagonal $\Sigma \ge 0$ such that
            \[
              A = U \Sigma V^\top, \qquad \sigma_i^2 \text{ are eigenvalues of } A^\top A.
            \]
          </div>
          <details>
            <summary>Proof sketch (via $A^\top A$)</summary>
            The matrix $A^\top A$ is symmetric PSD. Choose orthonormal eigenvectors $v_i$ with eigenvalues $\sigma_i^2 \ge 0$. Set $u_i = A v_i / \sigma_i$ for $\sigma_i &gt; 0$ (extend to a basis for the nullspace if needed). Then $A = \sum_i \sigma_i u_i v_i^\top = U \Sigma V^\top$.
          </details>
          <div class="ex">
            Example: $A = \begin{bmatrix}3 & 0\\ 0 & 1\end{bmatrix}$. Then $A^\top A = \operatorname{diag}(9, 1)$, so $\Sigma = \operatorname{diag}(3, 1)$ and $U = V = I$.
          </div>
        </section>

        <section class="card">
          <h2>5) Polar Decomposition: $A = U P$</h2>
          <p><strong>Background &amp; usage.</strong> Splits a linear map into an orthogonal factor $U$ and a symmetric stretch $P$, useful in mechanics and manifold optimization.</p>
          <div class="formula">
            For $A \in \mathbb{R}^{m \times n}$ with full column rank (if $m \ge n$), define $P = (A^\top A)^{1/2} \succeq 0$ and $U = A P^{-1}$. Then
            \[
              A = U P, \qquad U^\top U = I.
            \]
          </div>
          <details>
            <summary>Proof sketch (from the SVD)</summary>
            Let $A = U_0 \Sigma V^\top$. Then $A^\top A = V \Sigma^2 V^\top$, so $P = V \Sigma V^\top$. Consequently $U = A P^{-1} = U_0 V^\top$ with $U^\top U = I$, and $A = U P$.
          </details>
          <div class="ex">
            Example: $A = \begin{bmatrix}0 & 1\\ 1 & 0\end{bmatrix}$. Since $A^\top A = I$, $P = I$ and $U = A$, giving $A = U P$ with $U$ orthogonal.
          </div>
        </section>

      </div>
    </article>

    <nav class="chapter-nav bottom">
      <a href="page5Calculus.html">&lt;- Previous Chapter</a>
      <a class="index-link" href="index.html">Back to Index</a>
      <a href="page7AdvancedConcepts.html">Next Chapter -&gt;</a>
    </nav>

    <footer class="site-footer">
      <p>Digital Notes Series - Chapter 6</p>
    </footer>
  </main>
</body>
</html>

