<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Chapter 9 - Applications in Machine Learning</title>
  <link rel="stylesheet" href="styles.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['\\[', '\\]'], ['$$','$$']],
        processEscapes: true,
        tags: 'ams'
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <main class="book-page">
    <header class="book-header">
      <p class="lead">Chapter 9</p>
      <h1>Applications in Machine Learning</h1>
      <p class="muted">Least squares, regularization, PCA, low rank approximation, and logistic regression curvature.</p>
    </header>

    <nav class="chapter-nav">
      <a href="page8Random.html">&lt;- Previous Chapter</a>
      <a class="index-link" href="index.html">Back to Index</a>
      <a href="page10more.html">Next Chapter -&gt;</a>
    </nav>

    <article>
      <div class="grid">

        <section class="card">
          <h2>1) Least Squares Solution</h2>
          <p><strong>Background &amp; usage.</strong> The classical linear regression estimator; appears in projection problems and data fitting.</p>
          <div class="formula">
            For $x^\star = \arg\min_x \|Ax - b\|_2^2$ with full column rank,
            \[
              A^\top A x^\star = A^\top b, \qquad x^\star = (A^\top A)^{-1} A^\top b.
            \]
          </div>
          <details>
            <summary>Proof</summary>
            Differentiate $f(x) = \|Ax - b\|_2^2 = (Ax - b)^\top (Ax - b)$. The gradient is $2 A^\top (Ax - b)$, so the first order condition $A^\top (Ax - b) = 0$ gives the normal equations. Full column rank makes $A^\top A$ invertible.
          </details>
          <div class="ex">
            Example: $A = \begin{bmatrix}1 & 1\\ 1 & 2\\ 1 & 3\end{bmatrix}$, $b = \begin{bmatrix}1\\ 2\\ 2\end{bmatrix}$. Then $A^\top A = \begin{bmatrix}3 & 6\\ 6 & 14\end{bmatrix}$, $A^\top b = \begin{bmatrix}5\\ 11\end{bmatrix}$, yielding $x^\star = \begin{bmatrix}1\\ 0\end{bmatrix}$.
          </div>
        </section>

        <section class="card">
          <h2>2) Ridge Regression Closed Form</h2>
          <p><strong>Background &amp; usage.</strong> Regularizes ill-conditioned problems; trades bias for variance reduction.</p>
          <div class="formula">
            The ridge minimizer solves
            \[
              x_\lambda = \arg\min_x \left( \|Ax - b\|_2^2 + \lambda \|x\|_2^2 \right) \Rightarrow (A^\top A + \lambda I) x_\lambda = A^\top b.
            \]
          </div>
          <details>
            <summary>Proof</summary>
            Differentiate the objective: $2 A^\top (Ax - b) + 2 \lambda x = 0$, giving $(A^\top A + \lambda I) x = A^\top b$. For $\lambda &gt; 0$ the matrix is SPD, guaranteeing a unique solution.
          </details>
          <div class="ex">
            Example: Using the $A, b$ above with $\lambda = 1$, $(A^\top A + \lambda I) = \begin{bmatrix}4 & 6\\ 6 & 15\end{bmatrix}$ and $(A^\top A + \lambda I) x = A^\top b$ gives $x_\lambda \approx \begin{bmatrix}0.96\\ 0.32\end{bmatrix}$.
          </div>
        </section>

        <section class="card">
          <h2>3) PCA and Eigen-Decomposition of Covariance</h2>
          <p><strong>Background &amp; usage.</strong> Finds orthogonal directions of maximal variance; used for dimensionality reduction and denoising.</p>
          <div class="formula">
            For centered data $X = [x_1, \dots, x_n]$, the covariance is $C = \tfrac{1}{n} X X^\top$. Principal components are eigenvectors of $C$ and the variance along $v$ is $v^\top C v$.
          </div>
          <details>
            <summary>Proof</summary>
            Maximize $v^\top C v$ subject to $\|v\| = 1$. This Rayleigh quotient is maximized by the top eigenvector. Orthogonal components follow by imposing orthogonality constraints and deflating.
          </details>
          <div class="ex">
            Example: $X = \begin{bmatrix}1 & -1\\ 0 & 0\end{bmatrix}$ (already centered) gives $C = \tfrac{1}{2} \begin{bmatrix}2 & 0\\ 0 & 0\end{bmatrix} = \operatorname{diag}(1, 0)$. The top component is $e_1$ with variance $1$.
          </div>
        </section>

        <section class="card">
          <h2>4) Optimal Low Rank Approximation</h2>
          <p><strong>Background &amp; usage.</strong> Eckart&ndash;Young&ndash;Mirsky says the truncated SVD is the best rank-$k$ approximation in 2- and Frobenius norms.</p>
          <div class="formula">
            If $A = U \Sigma V^\top$ and $\Sigma_k$ keeps the top $k$ singular values,
            \[
              A_k = U \Sigma_k V^\top = \arg\min_{\operatorname{rank}(B) \le k} \|A - B\|_F,
            \]
            with error $\|A - A_k\|_F^2 = \sum_{i &gt; k} \sigma_i^2$.
          </div>
          <details>
            <summary>Proof sketch</summary>
            Unitary invariance of the Frobenius norm reduces the problem to the diagonal matrix $\Sigma$. Zeroing all but the largest $k$ entries yields the optimal approximation, and the error follows from Pythagorean decomposition of singular directions.
          </details>
          <div class="ex">
            Example: For $A = \operatorname{diag}(5, 2, 1)$ with $k = 1$, $A_1 = \operatorname{diag}(5, 0, 0)$ and $\|A - A_1\|_F = \sqrt{2^2 + 1^2} = \sqrt{5}$.
          </div>
        </section>

        <section class="card">
          <h2>5) Logistic Regression: Gradient and Hessian</h2>
          <p><strong>Background &amp; usage.</strong> Shows the log-likelihood is concave and underpins Newton or IRLS methods.</p>
          <div class="formula">
            For features $a_i$, labels $y_i \in \{0,1\}$, probabilities $p_i = \sigma(a_i^\top w)$, the log-likelihood $\ell(w) = \sum [y_i \log p_i + (1 - y_i) \log(1 - p_i)]$ satisfies
            \[
              \nabla \ell = A^\top (y - p), \qquad \nabla^2 \ell = -A^\top W A \preceq 0,
            \]
            where $W = \operatorname{diag}(p_i (1 - p_i)) \succeq 0$.
          </div>
          <details>
            <summary>Proof</summary>
            Because $\partial p_i / \partial w = p_i (1 - p_i) a_i$, differentiating $\ell$ yields the stated gradient. Differentiating once more introduces the diagonal weights $W$, giving the negative semidefinite Hessian.
          </details>
          <div class="ex">
            Example: $A = \begin{bmatrix}1 & 0\\ 1 & 1\end{bmatrix}$, $y = [1, 0]^\top$, $w = [0, 0]^\top$. Then $p = [0.5, 0.5]^\top$, $\nabla \ell = [0.5, -0.5]^\top$, and $\nabla^2 \ell = -A^\top W A = -\begin{bmatrix}0.5 & 0.25\\ 0.25 & 0.25\end{bmatrix} \preceq 0$.
          </div>
        </section>

      </div>
    </article>

    <nav class="chapter-nav bottom">
      <a href="page8Random.html">&lt;- Previous Chapter</a>
      <a class="index-link" href="index.html">Back to Index</a>
      <a href="page10more.html">Next Chapter -&gt;</a>
    </nav>

    <footer class="site-footer">
      <p>Digital Notes Series - Chapter 9</p>
    </footer>
  </main>
</body>
</html>

