<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Applications in Machine Learning — Wide Layout</title>
<script>
  window.MathJax = {
    tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['\\[','\\]'], ['$$','$$']], processEscapes: true, tags:'ams' },
    options: { skipHtmlTags:['script','noscript','style','textarea','pre','code'] }
  };
</script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<style>
:root { --bg:#fff; --fg:#0b1221; --muted:#5f6b7a; --card:#f7f9fc; --border:#e6ecf2; --accent:#0e7afe;}
body{font-family:system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial,sans-serif;color:var(--fg);background:var(--bg);margin:3rem;line-height:1.55;max-width:1500px;}
h1{margin-bottom:1rem;} h2{margin-top:2rem;}
.grid{display:grid;gap:2rem;grid-template-columns:1fr;}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:2rem 2.5rem;font-size:1.05rem;}
.formula{margin:.6rem 0 1rem 0;} .ex{background:#fff;border:1px dashed var(--border);border-radius:10px;padding:1rem;margin-top:.75rem;font-family:Consolas,"Courier New",monospace;font-size:1rem;overflow-x:auto;}
details{margin-top:.6rem;background:#fff;border:1px solid var(--border);border-radius:10px;padding:.75rem 1rem;}
details>summary{cursor:pointer;font-weight:600;color:var(--accent);} details>summary::-webkit-details-marker{display:none;}
.muted{color:var(--muted);}
</style>
</head>
<body>
<h1>Applications in Machine Learning</h1>
<p class="muted">Least squares, ridge regression, PCA, optimal low-rank approximation, and logistic regression curvature.</p>

<div class="grid">

<section class="card">
  <h2>1) Least Squares Solution (Normal Equations)</h2>
  <p><strong>Background & usage.</strong> Core linear regression estimator; also appears in projection problems and data fitting.</p>
  <div class="formula">
    For \(x^\star=\arg\min_x \|Ax-b\|_2^2\) with full column rank,
    \[
      A^\top A x^\star = A^\top b,\qquad x^\star=(A^\top A)^{-1}A^\top b.
    \]
  </div>
  <details>
    <summary>Proof</summary>
    Differentiate \(f(x)=\|Ax-b\|^2=(Ax-b)^\top(Ax-b)\): \(\nabla f=2A^\top(Ax-b)=0\Rightarrow A^\top A x=A^\top b\). Invertibility follows from full column rank.
  </details>
  <div class="ex">
    Example: \(A=\begin{bmatrix}1&1\\1&2\\1&3\end{bmatrix}\), \(b=\begin{bmatrix}1\\2\\2\end{bmatrix}\).  
    \(A^\top A=\begin{bmatrix}3&6\\6&14\end{bmatrix}\), \(A^\top b=\begin{bmatrix}5\\11\end{bmatrix}\).  
    Solve to get \(x^\star=\begin{bmatrix}1\\0\end{bmatrix}\).
  </div>
</section>

<section class="card">
  <h2>2) Ridge Regression Closed Form</h2>
  <p><strong>Background & usage.</strong> Regularizes ill-conditioned problems; trades bias for variance reduction.</p>
  <div class="formula">
    Ridge minimizer:
    \[
      x_\lambda=\arg\min_x \big(\|Ax-b\|_2^2+\lambda\|x\|_2^2\big)
      \ \Rightarrow\ 
      (A^\top A+\lambda I)x_\lambda=A^\top b.
    \]
  </div>
  <details>
    <summary>Proof</summary>
    Differentiate objective: \(2A^\top(Ax-b)+2\lambda x=0\Rightarrow (A^\top A+\lambda I)x=A^\top b\). SPD for \(\lambda>0\) ensures unique solution.
  </details>
  <div class="ex">
    Example: same \(A,b\) as above, \(\lambda=1\):  
    \(A^\top A+\lambda I=\begin{bmatrix}4&6\\6&15\end{bmatrix}\).  
    Solve \(\begin{bmatrix}4&6\\6&15\end{bmatrix}x=\begin{bmatrix}5\\11\end{bmatrix}\Rightarrow x_\lambda=\begin{bmatrix}0.96\\0.32\end{bmatrix}\) (rounded).
  </div>
</section>

<section class="card">
  <h2>3) PCA as Eigen-Decomposition of Covariance</h2>
  <p><strong>Background & usage.</strong> Finds orthogonal directions of maximal variance; used for dimensionality reduction and denoising.</p>
  <div class="formula">
    For centered data \(X=[x_1,\dots,x_n]\), covariance \(C=\frac1n XX^\top\).  
    Principal components are eigenvectors of \(C\); explained variance along \(v\) is \(v^\top C v\).
  </div>
  <details>
    <summary>Proof</summary>
    Maximize \(v^\top C v\) s.t. \(\|v\|=1\) → Rayleigh quotient; the maximizer is the top eigenvector. Orthogonal PCs follow by deflation/constraints.
  </details>
  <div class="ex">
    Example: \(X=\begin{bmatrix}1&-1\\0&0\end{bmatrix}\) (already centered).  
    \(C=\frac12\begin{bmatrix}2&0\\0&0\end{bmatrix}=\operatorname{diag}(1,0)\).  
    Top PC is \(e_1\) with variance 1.
  </div>
</section>

<section class="card">
  <h2>4) Optimal Low-Rank Approximation (Eckart–Young–Mirsky)</h2>
  <p><strong>Background & usage.</strong> Gives best rank-$k$ approximation under Frobenius/2–norms — foundation of truncated SVD.</p>
  <div class="formula">
    If \(A=U\Sigma V^\top\) and \(\Sigma_k\) keeps top \(k\) singular values,
    \[
      A_k=U\Sigma_k V^\top=\arg\min_{\operatorname{rank}(B)\le k}\|A-B\|_F,
      \quad \|A-A_k\|_F^2=\sum_{i>k}\sigma_i^2.
    \]
  </div>
  <details>
    <summary>Proof sketch</summary>
    Unitary invariance of Frobenius norm reduces to diagonal case. Best approximation zeros out small singular values; Pythagorean decomposition of singular directions yields the error formula.
  </details>
  <div class="ex">
    Example: \(A=\operatorname{diag}(5,2,1)\), \(k=1\) ⇒ \(A_1=\operatorname{diag}(5,0,0)\), error \(\sqrt{2^2+1^2}=\sqrt{5}\).
  </div>
</section>

<section class="card">
  <h2>5) Logistic Regression: Gradient & Hessian (PSD)</h2>
  <p><strong>Background & usage.</strong> Ensures convexity for maximum-likelihood; Newton/IRLS uses the Hessian structure.</p>
  <div class="formula">
    For labels \(y_i\\in\\{0,1\}\), \(p_i=\sigma(a_i^\top w)\), log-likelihood \(\ell(w)=\sum[y_i\log p_i+(1-y_i)\log(1-p_i)]\),
    \[
      \nabla \ell = \sum a_i(y_i-p_i)=A^\top(y-p),\qquad
      \nabla^2 \ell = -A^\top W A \preceq 0,
    \]
    where \(W=\operatorname{diag}(p_i(1-p_i))\\succeq0\).
  </div>
  <details>
    <summary>Proof</summary>
    \(\partial p_i/\partial w = p_i(1-p_i)a_i\). Differentiate \(\ell\) to get gradient; Jacobian of \(p\) gives Hessian \(-A^\top W A\), negative semidefinite, hence \(-\ell\) convex.
  </details>
  <div class="ex">
    Example: \(A=\begin{bmatrix}1&0\\1&1\end{bmatrix}\), \(y=[1,0]^\top\), \(w=[0,0]^\top\).  
    \(p=\sigma(Aw)=[0.5,0.5]^\top\).  
    \(\nabla \ell=A^\top(y-p)=[0.5,-0.5]^\top\).  
    \(W=\operatorname{diag}(0.25,0.25)\Rightarrow \nabla^2\ell=-A^\top W A=-\begin{bmatrix}0.5&0.25\\0.25&0.25\end{bmatrix}\preceq0\).
  </div>
</section>

</div>
</body>
</html>
